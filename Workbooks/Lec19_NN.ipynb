{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tITJ8HYGS__J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#load and visualize the dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq-_pmyRphTh",
        "outputId": "98e98ec0-db1d-4215-8555-855b6f66aeec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we’ve seen what our data looks like, we want to convert it into arrays for our machine to process\n"
      ],
      "metadata": {
        "id": "PvitT_BLTWye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assign input features to X and label to Y\n"
      ],
      "metadata": {
        "id": "F-kNaTuvTdyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "# I want to use the code in ‘preprocessing’ within the sklearn package.\n",
        "#Then, we use a function called the min-max scaler, which scales the dataset\n",
        "# so that all the input features lie between 0 and 1 inclusive:\n"
      ],
      "metadata": {
        "id": "lJVFEQFITvmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split dataset into train and test set\n",
        "\n",
        "# split test set further dataset into val and test set\n",
        "\n"
      ],
      "metadata": {
        "id": "_4H3sm34UCKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)\n"
      ],
      "metadata": {
        "id": "yu64Bap_UXc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "metadata": {
        "id": "BgeoNZq_UqTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# built model\n"
      ],
      "metadata": {
        "id": "7Dt_pmxnUskD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile your model**\n",
        "\n",
        "We put the following settings inside the brackets after model.compile:\n",
        "\n",
        "optimizer='sgd'\n",
        "\n",
        "‘sgd’ refers to stochastic gradient descent (over here, it refers to mini-batch gradient descent), which we’ve seen in Class lecture.\n",
        "\n",
        "loss='binary_crossentropy'\n",
        "\n",
        "The loss function for outputs that take the values 1 or 0 is called binary cross entropy.\n",
        "\n",
        "metrics=['accuracy']\n",
        "\n"
      ],
      "metadata": {
        "id": "QzBJmZFKU6Ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile your model\n"
      ],
      "metadata": {
        "id": "DqR5KKOEU0FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now train your model using fit\n"
      ],
      "metadata": {
        "id": "apQtR7n1VNkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the loss as the first element and the accuracy as the second element. To only output the accuracy, simply access the second element (which is indexed by 1, since the first element starts its indexing from 0)."
      ],
      "metadata": {
        "id": "pKLXd3ydVmOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate your model using model.evaluate\n",
        "\n"
      ],
      "metadata": {
        "id": "9dFnNHBMVU1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e79Jf7f2VvGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can do the same to plot our training accuracy and validation accuracy with the code below:\n",
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b_UNkD6cV4kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now add more layers and use adam optimizer and plot the training and validations curves again, report accuracy on test set\n",
        "\n"
      ],
      "metadata": {
        "id": "MWxDAnIzWoN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now add more layers and use regularization and dropout, plot the training and validations curves again, report accuracy on test set\n",
        "\n"
      ],
      "metadata": {
        "id": "vVfWz2wTXLw5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}